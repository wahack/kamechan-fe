/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_URL\n  }\n}\n\nclient<llm> QwQ_3 {\n  provider openai-generic\n  options {\n    model  \"qwen/qwen3-235b-a22b\"\n    max_tokens 20000\n    temperature 0\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    // allowed_roles [\"user\", \"assistant\"]\n  }\n}\n\nclient<llm> Claude3_7 {\n  provider openai-generic\n  options {\n    model  \"anthropic/claude-3.7-sonnet\"\n    max_tokens 20000\n    temperature 0\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    // allowed_roles [\"user\", \"assistant\"]\n  }\n}\nclient<llm> Gemini2_0FlashLite {\n  provider openai-generic\n  retry_policy Constant\n  options {\n    model  \"google/gemini-2.0-flash-lite-001\"\n    max_tokens 20000\n    temperature 0\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    // allowed_roles [\"user\", \"assistant\"]\n  }\n}\nclient<llm> Gemini2_0Flash {\n  provider openai-generic\n  retry_policy Constant\n  options {\n    model  \"google/gemini-2.0-flash-001\"\n    max_tokens 20000\n    temperature 0\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    // allowed_roles [\"user\", \"assistant\"]\n  }\n}\nclient<llm> Gemini2_5FlashPreview {\n  provider openai-generic\n  retry_policy Constant\n  options {\n    model  \"google/gemini-2.5-flash-preview\"\n    max_tokens 20000\n    temperature 0\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    // allowed_roles [\"user\", \"assistant\"]\n  }\n}\nclient<llm> Gemini2_5ProPreview {\n  provider openai-generic\n  retry_policy Constant\n  options {\n    model  \"google/gemini-2.5-pro-preview\"\n    max_tokens 20000\n    temperature 0\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    // allowed_roles [\"user\", \"assistant\"]\n  }\n}\nclient<llm> DeepSeekR1OpenRouter {\n  provider openai-generic\n  options {\n    model  \"deepseek/deepseek-r1\"\n    max_tokens 20000\n    temperature 0\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    // allowed_roles [\"user\", \"assistant\"]\n  }\n}\n\n\nclient<llm> GPT4oMini {\n  provider openai\n  retry_policy Constant\n  options {\n    model \"gpt-4o-mini\"\n    max_tokens 16384\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://api.gptsapi.net/v1\"\n  }\n}\n\nclient<llm> DeepSeek {\n  \n  provider openai-generic\n  options {\n    model  \"deepseek-r1\"\n    max_tokens 0\n    temperature 0\n    api_key env.OPENROUTER_API_KEY\n    base_url \"https://api.gptsapi.net/v1\"\n  }\n}\n\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}\n",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.89.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "types.baml": "enum Role {\n  User\n  Assistant\n  System\n  Tool\n}\n\nclass Message {\n  role string\n  content string\n}\n\nclass DynamicData{\n  @@dynamic\n}",
  "workflow/ChatClassify.baml": "enum ChatClassification {\n  \n  Workflow @alias(\"k2\")\n  @description(#\"\n    workflows,  actions or complicated tasks\n    anylysis or lookup any info/data like news, market, tokens, social, transactions, defi, onchain data, web3 knowledge, and so on.\n    do any action like trading, depositing, staking, lending, interact with contract / wallet, software integration, and so on.\n    • Cross-protocol interactions (DEX → CEX → bridge → yield farm)\n    • Multi-condition logic with nested branching\n    • Scheduled/recurring automations\n   \n  \"#)\n\n  NotWorflow @alias(\"k1\")\n  @description(#\"\n    Casual and relaxed conversation,  not related to Info/action topic. Please ask about workflow building.\n  \"#)\n\n  ScheduledWorkflow @alias(\"k3\")\n  @description(#\"\n    daily task,  Scheduled workflow,  planned tasks\n  \"#)\n\n}\n\n\nfunction ClassifyMessage (messages: Message[]) -> ChatClassification {\n  client Gemini2_0FlashLite\n\n  prompt #\"\n  {{ ctx.output_format }}\n  \n  # Role\n   You a network of blockchain agents. Classify the  messages\n    \n  {% for message in messages %}\n    {{ _.role(message.role) }}\n    {{ message.content }}\n  {% endfor %}\n\n  \"#\n}\n\ntest TestClassifyMessage {\n  functions [ClassifyMessage]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"staking token sui to blue\"\n      }\n    ]\n  }\n}",
  "workflow/FixWorkflow.baml": "function FixdWorkflow ( messages: Message[], outputExamples: string[], workflow: string, errorMessage: string, services: WorkflowNodeDocs[], nodesDoc: string) -> LookupNodeDocTool | NodeExecuteTool | WorkflowBuilt {\n  client QwQ_3\n  prompt #\"\n    {{ ctx.output_format(prefix=\"**Response Format Guidance**: Construct your answer using this schema:\") }}\n    # Role\n    Senior n8n Workflow Architect specializing in process automation. \n    Specialization: Correct the input n8n workflow according to error messages\n   \n    # node integrated\n    {% for service in services %}\n    - {{service.name}}: {{service.description}}\n    {% endfor %}\n\n    # partial node docs provided\n    {{nodesDoc}}\n\n\n    # Using the LookupDocTool\n    - nodeEmbeddingsDoc: a vector database that is filled with information about all the nodes usage\n    - docType: nodeEmbeddingsDoc\n    - **queryPhrases** format rules: using format like nodeName: enum(get, list, find, create, update, delete) {resource description}. Examples:\n      - okxApi:get btc realtime price\n      - insidexApi:list top trading counts\n      - cetusSdk: get pool by coins\n\n    # Using the NodeExecuteTool\n    - call this tool to check or validate the node's output data \n\n    # Think around how the base workflow works and user's intention, using the LookupDocTool to check the input and output, and correct the workflow error\n\n     # Workflow examples\n      {% for example in outputExamples %}\n      -  {{ example }}\n      {% endfor %}\n\n    # Input\n    - original n8n workflow\n        {{workflow}}\n    \n    - Error message\n        {{ errorMessage }}\n\n    {% for message in messages %}\n      {{ _.role(message.role) }}\n      \n      {{ message.content }}\n    {% endfor %}\n\n\n  \"#\n}\n",
  "workflow/ParseN8nData.baml": "function ParseN8n (str: string) -> N8nWorkflow {\n  client QwQ_3\n  prompt #\"\n    {{ ctx.output_format(prefix=\"**Response Format Guidance**: Construct your answer using this schema:\") }}\n    parse to string to Json\n\n    {{str}}\n\n  \"#\n}",
  "workflow/RequirementsFlowchart.baml": "function RequirementsFlowchart ( messages: Message[], services: WorkflowNodeDocs[], outputExamples: string[]) ->  ThinkTool | LookupNodeDocTool | NodeExecuteTool | WorkflowIntention {\n  client Gemini2_5ProPreview\n  prompt #\"\n    {{ ctx.output_format(prefix=\"**Response Format Guidance**: Construct your answer using this schema:\") }}\n    # Objective\n    Based on user requirements, Design a workflow in mermaid flowchart format using available node services.\n      \n\n    # node integrated\n    {% for service in services %}\n    - {{service.name}}: {{service.description}}\n    {% endfor %}\n \n    # Rules\n    - before calling **Node** , lookup the  **node knowledge database** first\n\n    # Using the ThinkTool\n    Before taking any action or responding to the user after receiving tool results, use the ThinkTool as a scratchpad to:\n    - List the specific rules that apply to the current request\n    - Check if all required information is collected\n    - Verify that the planned action complies with all policies\n    - Iterate over tool results for correctness\n\n    # Using the LookupNodeDocTool\n    - a vector database that is filled with information about all the node services\n    - **queryPhrases** recommend using format like nodeName: enum(get, list, find, create, update, delete) {resource description}. Examples:\n      - okx:get btc realtime price\n      - insidex:list top trading counts\n      - cetus: get pool by coins\n    \n    # Using the NodeExecuteTool\n    - call this tool to check or validate the node's output data\n    \n    # Mermaid Parameter Encoding Rules:\n      - graph node syntax: A[\"`node name<br/>title<br/>`\"]\n      - example: A[\"`okx<br/>fetch BTC/USDT realtime price<br/>fetch BTC/USDT realtime price`\"]\n       \n    # Output examples\n    {% for example in outputExamples %}\n    -  {{ example }}\n    {% endfor %}\n\n    {% for message in messages %}\n      {{ _.role(message.role) }}\n      \n      {{ message.content }}\n    {% endfor %}\n  \"#\n}",
  "workflow/RequirementsNodes.baml": "function RequirementsNodes (messages: Message[], services: WorkflowNodeDocs []) -> RequirementsNodesResult {\n  client Gemini2_0FlashLite\n  prompt #\"\n    {{ ctx.output_format(prefix=\"**Response Format Guidance**: Construct your answer using this schema:\") }}\n    You are a skilled assistant in the cryptocurrency/Web3 blockchain field, proficient in various on-chain and off-chain operations.\n    Your objective is to analyze user requirements and select multiple potentially relevant services from the following service options\n    \n    # services options\n    {% for service in services %}\n    - {{service.name}}: {{service.description}}\n    {% endfor %}\n\n    # Rules\n    Infer the relevant services that may be used to address user needs based on the service description. And output your reasoning.\n\n    \n    {% for message in messages %}\n      {{ _.role(message.role) }}\n      {{ message.content }}\n    {% endfor %}\n\n  \"#\n}",
  "workflow/generateN8n.baml": "function GenerateN8n (messages: Message[], services: WorkflowNodeDocs[], shots: string[]) ->  LookupNodeDocTool | NodeExecuteTool | N8nWorkflow {\n  client DeepSeekR1OpenRouter\n  prompt #\"\n    {{ ctx.output_format(prefix=\"**Response Format Guidance**: Construct your answer using this schema:\") }}\n    # Role\n    Senior n8n Workflow Architect specializing in process automation. \n    Specialization: Generate  production-ready n8n workflows based on user requirements\n    \n    # node integrated\n    {% for service in services %}\n    - {{service.name}}: {{service.description}}\n    {% endfor %}\n \n    # Using the LookupNodeDoc Tool\n    - a vector database that is filled with information about all the nodes usage\n    - **queryPhrases** recommend rule to improve search accuracy: using format like nodeName: enum(get, list, find, create, update, delete) {resource description}. Examples:\n      - okx: get btc realtime price\n      - insidex:list top trading counts\n      - cetus: get pool by coins\n\n    # Using the NodeExecuteTool\n    - call this tool to check or validate the node's output data \n\n    **Workflow Validation & Dependency Resolution Guide**\n\n    1. **Sequential Node Inspection**  \n      Examine nodes step-by-step from the beginning, strictly validating each node's parameters and outputs against Node documentation or actual interface invocations.\n\n    2. **Node Rules**  \n    - workflow MUST start with the trigger node \"n8n-nodes-base.executeWorkflowTrigger\", and the node name is fixed as 'Start' \n    - use **userInteraction** node to collect user config\n    - Node names must concisely describe the node's action (≤6 words)\n\n    # Case examples\n    {% for example in shots %}\n    -  {{ example }}\n    {% endfor %}\n\n    {% for message in messages %}\n      {{ _.role(message.role) }}\n      \n      {{ message.content }}\n    {% endfor %}\n  \"#\n}\n",
  "workflow/type.baml": "class ThinkTool {\n  tool_name \"ThinkTool\"  @description(#\"\n    Markdown Content. Use the tool to think about something. It will not obtain new information or change the database, but just append the thought to the log. Use it when complex reasoning or some cache memory is needed.\n  \"#) @stream.done\n  thought string @description(#\"\n    A thought to think about\n  \"#)\n}\n\nclass RequirementsNodesResult {\n  reasoning string @description(#\"\n    describe the your thoughts\n  \"#)\n  nodes string[] @description(#\"\n    service names\n  \"#)\n}\n\nclass WorkflowIntention {\n  // tool_name \"WorkflowIntention\"\n  reasoning string @description(#\"\n    describe the flowchart process in detail\n  \"#)\n  mermaidFlowchart string @description(#\"\n    prefix with **graph TB***, show the step by step process\n  \"#)\n}\n\nclass WorkflowBuilt {\n  // description string @description(#\"\n  //   Provide a structured breakdown of the workflow with granular steps, formatted in Markdown\n  // \"#)\n  workflowData string @description(#\"\n     generated json stringify n8n workflow \n  \"#)\n}\n\n\nclass LookupNodeDocTool {\n  tool_name \"LookupNodeDoc\" @description(\"Use This tool to lookup embedding vector database of nodes\") @stream.done\n  queryPhrases string @description(\"The phrases used to search for documetation\")\n}\n\nclass NodeExecuteTool {\n  tool_name \"NodeExecuteTool\" @description(\"call this tool to execute the node\") @stream.done\n  // reasoning string @description(\"explanation of why this tool was chosen use complete sentences\")\n  node string @description(#\"\n    name of the node, eg: cetus\n  \"#)\n  purpose string @description(#\"\n    why execute this node\n  \"#)\n  parameters string @description(#\"\n    parameters json stringify format\n  \"#)\n}\n\nclass WorkflowNodeDocs {\n  name string\n  description string\n}\n\nclass BNodeConnectionNode {\n  node string\n  type string\n  index int\n}\nclass BNodeConnection {\n  main BNodeConnectionNode[][]\n}\nclass BWorkflowNodesParams {\n  inputSource string?\n  params string?\n  functionCode string?\n}\nclass BWorkflowNodes {\n  type string\n  name string\n  parameters BWorkflowNodesParams\n}\nclass BCon {\n  @@dynamic\n}\nclass N8nWorkflow {\n  nodes BWorkflowNodes[]\n  connections BCon\n}",
}
export const getBamlFiles = () => {
    return fileMap;
}